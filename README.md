# Examen Kubernetes - Instructions

Cet examen √©value votre ma√Ætrise des concepts fondamentaux de Kubernetes en d√©ployant une application fullstack (backend FastAPI + frontend React + base de donn√©es) de mani√®re progressive et professionnelle.

## Comp√©tences √©valu√©es

Cet examen √©value les comp√©tences suivantes :
- D√©ploiement d'une application multi-services sur Kubernetes
- Gestion de la configuration et des secrets de mani√®re s√©curis√©e
- Exposition d'une application via Ingress avec HTTPS
- Observation et monitoring d'une application avec Prometheus et Grafana
- Logging √† travers la stack OpenSearch, Fluentbit et OpeanSearch Dashboards

## Pr√©requis

- Cluster Kubernetes fonctionnel (Minikube recommand√©)
- `kubectl` install√© et configur√©
- `helm` install√©
- Docker install√© pour builder les images
- **Chargement des images dans Minikube** : 
  - Si vous utilisez Minikube, vous pouvez charger les images directement dans le cluster avec `minikube image load <image-name>` ou sinon les pousser vers un registry Docker
  - Exemple : `minikube image load exam-kubernetes-backend:latest`
  - Alternative : Utiliser un registry Docker (Docker Hub, ou registry local avec Minikube)

### Optionnel (Point bonus)

**D√©ploiement via Terraform/OpenTofu** : Des points suppl√©mentaires seront attribu√©s si tout le d√©ploiement (ou une partie significative) est r√©alis√© via Terraform ou OpenTofu au lieu de manifests YAML bruts ou Helm. Vous devrez fournir le code Terraform/OpenTofu et expliquer votre approche.

## Pr√©paration des images Docker

Avant de d√©ployer l'application sur Kubernetes, vous devez :

1. **Builder les images Docker** du backend et du frontend
   - **Important pour le frontend** : Builder avec `VITE_API_BASE_URL=""` (vide) pour que les redirections fonctionnent sur Kubernetes :
     ```bash
     docker build -t exam-kubernetes-frontend:latest \
       --build-arg VITE_API_BASE_URL="" ./frontend
     ```
   - Pour le backend, builder normalement :
     ```bash
     docker build -t exam-kubernetes-backend:latest ./backend
     ```

2. **Charger les images dans votre cluster Kubernetes**
   
   **Option A : Avec Minikube (recommand√© pour le TP)**
   - Utiliser `minikube image load` pour charger les images directement dans Minikube :
     ```bash
     minikube image load exam-kubernetes-backend:latest
     minikube image load exam-kubernetes-frontend:latest
     ```
   - Dans vos manifests, utiliser directement le nom de l'image (sans registry) :
     ```yaml
     image: exam-kubernetes-backend:latest
     image: exam-kubernetes-frontend:latest
     ```
   
   **Option B : Avec un registry Docker**
   - Pousser les images vers un registry Docker de votre choix (ex: Docker Hub) :
     ```bash
     docker tag exam-kubernetes-backend:latest <votre-registry>/exam-kubernetes-backend:latest
     docker push <votre-registry>/exam-kubernetes-backend:latest
     ```
   - Dans vos manifests, utiliser le nom complet avec le registry :
     ```yaml
     image: <votre-registry>/exam-kubernetes-backend:latest
     ```

3. **Utiliser ces images dans vos manifests** : Lors de la cr√©ation de vos Deployments backend et frontend, vous devrez sp√©cifier les noms d'images correspondant √† la m√©thode choisie (Minikube ou registry Docker)

---

## PHASE 1 ‚Äî Kubernetes Core (fondamentaux)

### Objectif
Rendre l'application fonctionnelle et stable

### üìù Travaux demand√©s

1. **Cr√©er un namespace d√©di√©**
   - Cr√©er un namespace **todolist** pour isoler l'application
   - Utiliser ce namespace pour tous les d√©ploiements suivants

2. **D√©ployer les composants**
   - **Backend** : Cr√©er un Deployment pour le backend FastAPI
   - **Frontend** : Cr√©er un Deployment pour le frontend React
   - **Note** : Le backend affichera des erreurs de connexion √† la base de donn√©es car celle-ci n'est pas encore d√©ploy√©e (√©tape 8). C'est normal et attendu. Le backend deviendra fonctionnel une fois la base de donn√©es d√©ploy√©e. 

3. **Exposer les services**
   - **Backend** : Exposer via Service ClusterIP
   - **Frontend** : Exposer via Service ClusterIP

4. **Ajouter les probes au backend**
   - `livenessProbe` sur `/health`
   - `readinessProbe` sur `/ready`

5. **Configurer les ressources**
   - Ajouter `resources.requests` et `resources.limits` pour tous les conteneurs :
     - **Backend** : requests (CPU: 100m, Memory: 128Mi), limits (CPU: 500m, Memory: 512Mi)
     - **Frontend** : requests (CPU: 50m, Memory: 64Mi), limits (CPU: 200m, Memory: 256Mi)

6. **Externaliser la configuration**
   - Cr√©er une **ConfigMap** pour la configuration non sensible :
     - **Backend** : `APP_NAME`, `APP_VERSION`, `DB_HOST` (utiliser le FQDN du service : `<service-name>.<namespace>.svc.cluster.local`), `DB_PORT`, `DB_NAME`
     - **Frontend** : `VITE_API_BASE_URL` (vide), `BACKEND_URL` (URL du backend pour Nginx, ex: `http://backend:8000`)
   - Cr√©er un **Secret** pour les informations sensibles :
     - **Backend** : `DB_USER`, `DB_PASSWORD`
   - **Optionnel (Point bonus)** : Utiliser des secrets chiffr√©s avec **Sealed Secrets** ou **SOPS** au lieu de secrets en clair dans les YAML. Les secrets doivent √™tre chiffr√©s dans le code source et d√©chiffr√©s au d√©ploiement. **Important** : Vous devrez me fournir la cl√© de d√©chiffrement pour permettre la validation et le d√©chiffrement des secrets.

7. **Utiliser un ServiceAccount d√©di√© avec RBAC**
   - Cr√©er un ServiceAccount sp√©cifique √† l'application
   - L'associer aux Deployments
   - Cr√©er un **Role** permettant de cr√©er des pods dans le namespace `todolist`
   - Cr√©er un **RoleBinding** associant le Role au ServiceAccount
   - **Utilit√©** : Le backend expose un endpoint `/api/test-pod` qui cr√©e un pod de test. Cela permet de v√©rifier que le ServiceAccount a les bonnes permissions RBAC.

8. **D√©ployer la base de donn√©es avec volume persistant**
   - **Optionnel (Point bonus)** : Utiliser le CRD CockroachDB au lieu de PostgreSQL (plus complexe mais distribu√©)
   - Cr√©er un namespace **db** pour la base de donn√©es
   - Cr√©er un **PersistentVolumeClaim (PVC)** pour la base de donn√©es dans le namespace `db`
   - Cr√©er un **Secret** pour stocker les informations sensibles de la base de donn√©es :
     - Le nom de la base de donn√©es
     - L'utilisateur
     - Le mot de passe
     - **Notes** : Consultez la documentation officielle de l'image PostgreSQL ou CockraochDB pour conna√Ætre les noms exacts des variables d'environnement √† utiliser et les valeurs par d√©faut
   - D√©ployer la base de donn√©es dans le namespace `db` avec le volume attach√© et le Secret configur√©
   - **Important** : Le backend dans le namespace `todolist` devra acc√©der √† la base de donn√©es dans le namespace `db` via le FQDN du service : `<service-name>.<namespace>.svc.cluster.local` (ex: `postgres.db.svc.cluster.local`). De plus, regardez bien la documentation de postgres sur DockerHub pour voir quel volume mont√©.

9. **Initialiser la base de donn√©es**
   - Une fois la base de donn√©es d√©ploy√©e dans le namespace `db`, initialiser la table `items` :
     - **Note** : Le script `init_db.py` se trouve dans le dossier `resources/` du projet et n'est pas inclus dans l'image Docker. Vous devrez le rendre accessible au pod qui l'ex√©cutera.
     ```bash
     # Via kubectl cp (copier le script dans le pod backend)
     kubectl cp resources/init_db.py <pod-backend>:/tmp/init_db.py -n todolist
     kubectl exec -it <pod-backend> -n todolist -- python3 /tmp/init_db.py
     ```

### ‚úÖ Validation attendue

- [ ] Application accessible (frontend et backend fonctionnels)
- [ ] `/ready` retourne 503 si DB indisponible
- [ ] Aucun secret en clair dans les YAML (utilisation de Secret Kubernetes)
- [ ] Les pods d√©marrent correctement avec les probes configur√©es
- [ ] Les ressources sont limit√©es et les requests d√©finies
- [ ] Le bouton "Cr√©er un pod de test" dans le frontend fonctionne (teste les permissions RBAC du ServiceAccount)
- [ ] **Bonus** : Secrets chiffr√©s avec Sealed Secrets ou SOPS (si impl√©ment√©)
- [ ] **Bonus** : Utiliser le CRD CockroachDB au lieu de PostgreSQL (plus complexe mais distribu√©)

---

## PHASE 2 ‚Äî Observabilit√© (Prometheus & Grafana)

### Objectif
Observer l'application comme un SRE

### üìù Travaux demand√©s

1. **Installer kube-prometheus-stack via Helm**
   - Ajouter le repo Helm prometheus-community
   - Installer kube-prometheus-stack dans le namespace `monitoring`
   - **V√©rifier le r√©sultat de l'installation** : V√©rifier que tous les pods sont en cours d'ex√©cution avec `kubectl get pods -n monitoring`
   - **R√©cup√©rer le mot de passe Grafana** : Le mot de passe admin de Grafana est stock√© dans un Secret Kubernetes. Pour le r√©cup√©rer :
     ```bash
     kubectl get secret prometheus-grafana -n monitoring -o jsonpath="{.data.admin-password}" | base64 -d
     ```
     Notez ce mot de passe, vous en aurez besoin pour vous connecter √† Grafana.
   - Acc√©der √† Prometheus et Grafana via port-forward 

2. **Cr√©er un ServiceMonitor afin de scraper le backend**
   
   **Note** : Notre backend FastAPI expose un endpoint `/metrics` qui retourne les m√©triques au format Prometheus (Counter, Histogram, Gauge). Vous pouvez v√©rifier le code pour v√©rifier.

   **Qu'est-ce qu'un ServiceMonitor ?**
   
   Un ServiceMonitor est une Custom Resource Definition (CRD) fournie par le Prometheus Operator. Il permet de d√©clarer √† Prometheus comment scraper (collecter) les m√©triques d'un service Kubernetes, sans avoir √† modifier la configuration de Prometheus manuellement.
   
   **Fonctionnement** :
   - Le ServiceMonitor d√©crit quel Service Kubernetes scraper
   - Il sp√©cifie le port, le chemin (`/metrics`), et l'intervalle de scraping
   - Prometheus Operator d√©couvre automatiquement les ServiceMonitors et configure Prometheus pour les scraper
   - Cela permet une gestion d√©clarative des targets Prometheus via des ressources Kubernetes natives
   
   **Ce qu'il faut faire** :
   - Cr√©er un ServiceMonitor qui scrape l'endpoint `/metrics` de notre backend
   - Le ServiceMonitor doit s√©lectionner le Service backend via les labels
   - Sp√©cifier le port `http` (le Service backend doit avoir un port nomm√© `http`)
   - Sp√©cifier le chemin `/metrics` dans les endpoints
   - Utiliser un relabeling pour ajouter des labels personnalis√©s :
     - Ajouter le label `pod_name` √† partir de `__meta_kubernetes_pod_name`
     - Ajouter le label `namespace` √† partir de `__meta_kubernetes_namespace`
   - Ces labels permettront d'identifier plus facilement les m√©triques dans Prometheus et Grafana   

3. **V√©rifier l'exposition des m√©triques**
   - V√©rifier que le endpoint `/metrics` du backend est accessible
   - V√©rifier que Prometheus scrape les m√©triques en cliquant sur "Status" puis "Target Health" et rendez-vous tout en bas afin de voir votre serviceMonitor
   ![Operator](images/service-monitor.png)

4. **Cr√©er une PrometheusRule**
   - Utilisez ce manifest YAML
```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: backend-alerts
  namespace: todolist
  labels:
    app: backend
    # Ces labels sont importants pour que Prometheus d√©couvre la PrometheusRule
    # Ils doivent correspondre aux labels du RuleSelector dans la config Prometheus
spec:
  groups:
  - name: backend.rules
    interval: 30s
    rules:
    # Alerte : Backend down (pas de m√©triques depuis 2 minutes)
    # Note: Le job name peut varier selon la configuration Prometheus
    # V√©rifier le nom r√©el avec: up{namespace="todolist",service="backend"}
    - alert: BackendDown
      expr: |
        up{namespace="todolist",service="backend"} == 0
        or
        (absent(up{namespace="todolist",service="backend"}) == 1)
      for: 2m
      labels:
        severity: critical
        component: backend
      annotations:
        summary: "Backend is down"
        description: "Le backend n'expose plus de m√©triques depuis 2 minutes. Le service est probablement down."

    # Alerte : Taux d'erreurs 5xx √©lev√© (> 10% sur 5 minutes)
    - alert: HighErrorRate
      expr: |
        (
          sum(rate(http_requests_total{namespace="todolist",status=~"5.."}[5m])) by (route)
          /
          sum(rate(http_requests_total{namespace="todolist"}[5m])) by (route)
        ) * 100 > 10
      for: 5m
      labels:
        severity: warning
        component: backend
      annotations:
        summary: "Taux d'erreurs 5xx √©lev√© sur {{ $labels.route }}"
        description: "Le taux d'erreurs 5xx est de {{ $value | humanizePercentage }}% sur la route {{ $labels.route }} (seuil: 10%)"

    # Alerte : Latence p95 √©lev√©e (> 1 seconde)
    - alert: HighLatency
      expr: |
        histogram_quantile(0.95,
          sum(rate(http_request_duration_seconds_bucket{namespace="todolist"}[5m])) by (le, route)
        ) > 1
      for: 5m
      labels:
        severity: warning
        component: backend
      annotations:
        summary: "Latence p95 √©lev√©e sur {{ $labels.route }}"
        description: "La latence p95 est de {{ $value | humanize }}s sur la route {{ $labels.route }} (seuil: 1s)"

    # Alerte : Readiness probe √©choue
    - alert: BackendNotReady
      expr: |
        up{namespace="todolist",service="backend"} == 1 
        and 
        rate(http_requests_total{namespace="todolist",route="/ready",status="503"}[1m]) > 0
      for: 2m
      labels:
        severity: warning
        component: backend
      annotations:
        summary: "Backend not ready"
        description: "Le backend retourne 503 sur /ready, la base de donn√©es est probablement inaccessible."

    # Alerte : Trop de requ√™tes en cours (> 100)
    - alert: HighInflightRequests
      expr: sum(http_inflight_requests{namespace="todolist"}) > 100
      for: 2m
      labels:
        severity: warning
        component: backend
      annotations:
        summary: "Trop de requ√™tes en cours"
        description: "Il y a {{ $value }} requ√™tes en cours de traitement (seuil: 100)"
```

- Rendez vous dans "Alerts" et v√©rifiez dans votre groupe backend les diff√©rentes alertes. Vous pouvez supprimer de d√©ployement **backend** afin de voir que les alertes se d√©clenchent.

5. **Importer un dashboard Grafana**
   - Acc√©der √† Grafana (port-forward : `kubectl port-forward svc/prometheus-grafana 3000 -n monitoring`)
   - Se connecter avec les identifiants admin (mot de passe r√©cup√©r√© √† l'√©tape 1)
   - Aller dans **Dashboards ‚Üí Import**
   - Importer le dashboard JSON fourni dans le projet (fichier `resources/dashboard.json` √† la racine du projet)
   - Le dashboard contient les panneaux suivants :
     - **Total Requ√™tes (COUNT)** : nombre total de requ√™tes sur la p√©riode s√©lectionn√©e
     - **RPS par m√©thode HTTP** : taux de requ√™tes par seconde group√© par m√©thode (GET, POST, DELETE, etc.)
     - **Erreurs 500 dans le temps** : graphique temporel des erreurs 500 group√©es par route et m√©thode
     - **Latence p95** : 95√®me percentile de la latence par route
     - **Routes et m√©thodes HTTP** : tableau avec le nombre total de requ√™tes par route et m√©thode
     - **Nombre de requ√™tes dans le temps** : graphique temporel du nombre de requ√™tes group√©es par m√©thode, route et code de retour
   - Le dashboard inclut √©galement une variable **"Exclure /health et /ready"** (Oui/Non) permettant d'exclure ou d'inclure ces endpoints syst√®me des m√©triques
   - V√©rifier que la datasource Prometheus est correctement configur√©e dans le dashboard
   - V√©rifier que les panneaux affichent des donn√©es (g√©n√©rer du trafic si n√©cessaire)

![Grafana](images/grafana.png)

### ‚úÖ Validation attendue

- [ ] M√©triques visibles dans Prometheus (capture d'√©cran)
- [ ] ServiceMonitor fonctionnel et d√©couvert par Prometheus (capture d'√©cran)
- [ ] Dashboard Grafana fonctionnel avec tous les panneaux demand√©s (capture d'√©cran)
- [ ] Au moins une PrometheusRule cr√©√©e et visible (capture d'√©cran)

---

## PHASE 3 ‚Äî Hashicorp Vault (mots de passe dynamiques)

### Objectif
Utiliser Vault pour g√©n√©rer des mots de passe dynamiques pour PostgreSQL/CockRoachDB au lieu d'utiliser des secrets statiques

### Contexte

Au lieu d'utiliser un Secret Kubernetes avec un mot de passe statique pour PostgreSQL, nous allons utiliser le **Database Secrets Engine** de Vault pour g√©n√©rer des credentials dynamiques √† la demande. Cela permet :
- ‚úÖ **Rotation automatique** des mots de passe
- ‚úÖ **Cr√©ation de credentials temporaires** (TTL)
- ‚úÖ **Tra√ßabilit√©** via les audit logs de Vault
- ‚úÖ **S√©curit√© renforc√©e** : chaque pod peut avoir son propre credential

### Bonus

Vous pouvez utilisez le provider Vault sur OpenTofu/Terraform une fois celui-ci d√©ployer afin de cr√©er les secrets engine.

### üìù Travaux demand√©s

1. **Installer Hashicorp Vault dans Kubernetes**
   - Installer Vault via Helm (repo officiel HashiCorp)
   - D√©ployer Vault dans un namespace d√©di√© (ex: `vault`)
   - **Important** : Ajouter l'option `--set server.dev.enabled=true` lors de l'installation pour activer le mode dev
   - V√©rifier que les pods sont en cours d'ex√©cution
   - **Note** : En mode dev, Vault est automatiquement initialis√© et d√©verrouill√©. Le token root est `root`.

2. **Se connecter √† Vault (mode dev)**
   - Faire un port-forward vers Vault
   - Se connecter √† Vault avec le token root sur la console UI ou via CLI avec la commande `vault login root`

3. **Configurer le Database Secrets Engine**
   - Activer le Database Secrets Engine dans Vault
   - Configurer la connexion PostgreSQL :
     - Utiliser la connection URL (les √©l√©ments peuvent changer selon votre configuration): `postgresql://{{username}}:{{password}}@postgres.db.svc.cluster.local:5432/tpkubernetes`
     - Configurer la connexion dans Vault avec les credentials PostgreSQL existants
     - **Important** : Ne pas activer la rotation automatique du mot de passe (d√©sactiver la rotation)
   - Cr√©er un r√¥le Vault `ro` pour g√©n√©rer des credentials **dynamiques** :
     - D√©finir la TTL (ex: 1h) et le TTL max (ex: 24h)
     - **Creation statements** :
       ```sql
       CREATE ROLE "{{name}}" WITH LOGIN PASSWORD '{{password}}' VALID UNTIL '{{expiration}}';
       GRANT CONNECT ON DATABASE tpkubernetes TO "{{name}}";
       GRANT USAGE ON SCHEMA public TO "{{name}}";
       GRANT SELECT ON ALL TABLES IN SCHEMA public TO "{{name}}";
       ALTER DEFAULT PRIVILEGES IN SCHEMA public
       GRANT SELECT ON TABLES TO "{{name}}";
       ```
     - **Revocation statements** :
       ```sql
       REVOKE ALL PRIVILEGES ON ALL TABLES IN SCHEMA public FROM "{{name}}";
       REVOKE USAGE ON SCHEMA public FROM "{{name}}";
       REVOKE CONNECT ON DATABASE tpkubernetes FROM "{{name}}";
       DROP ROLE IF EXISTS "{{name}}";
       ```
     - **Rollback statements** :
       ```sql
       DROP ROLE IF EXISTS "{{name}}";
       ```

4. **Tester la g√©n√©ration de credentials dynamiques**
   - G√©n√©rer un mot de passe dynamique depuis Vault :
     ```bash
     vault read database/creds/ro
     ```
   - Noter le `username` et le `password` g√©n√©r√©s
   - D√©marrer un pod avec l'image `alpine/psql` pour tester la connexion :
     ```bash
     kubectl run psql-test --image=alpine/psql --rm -it --restart=Never \
       --env="<password-vault>" \
       -- psql \
       -h <service-db>.db.svc.cluster.local \
       -p 5432 \
       -U <username-vault> \
       -d <database-name>
     ```
   - Lorsque vous √™tes connect√©, tester les permissions read-only :
     ```sql
     -- V√©rifier que la lecture fonctionne
     SELECT * FROM items;
     
     -- Essayer de cr√©er un item (devrait √©chouer avec permissions read-only)
     INSERT INTO items (title) VALUES ('Test item');
     
     -- Essayer de supprimer l'item cr√©√© (devrait √©chouer avec permissions read-only)
     DELETE FROM items WHERE title = 'Test item';
     ```
   - V√©rifier que les op√©rations d'√©criture (INSERT, DELETE) √©chouent avec une erreur de permissions, confirmant que le r√¥le est bien en read-only

5. **Configurer l'authentification Kubernetes (Optionnel - Point bonus)**
   - Cette √©tape est optionnelle et permet d'aller plus loin dans l'utilisation de Vault
   - Activer le backend d'authentification Kubernetes dans Vault
   - Configurer le ServiceAccount de Vault pour s'authentifier aupr√®s de Kubernetes
   - Cr√©er un r√¥le Vault pour s'authentifier au cluster Kubernetes de mani√®re temporaire
   - **Note** : Cette configuration permet de cr√©er des r√¥les Vault qui peuvent s'authentifier au cluster Kubernetes de mani√®re temporaire, permettant ainsi aux applications de s'authentifier automatiquement aupr√®s de Vault via leur ServiceAccount Kubernetes sans avoir besoin de tokens statiques. C'est une pratique avanc√©e pour la gestion des secrets dynamiques.

### ‚úÖ Validation attendue

- [ ] Vault install√© et fonctionnel dans Kubernetes
- [ ] Database Secrets Engine configur√© pour PostgreSQL/CockroachDB
- [ ] Credentials dynamiques g√©n√©r√©s et test√©s avec le pod `alpine/psql` (permissions read-only v√©rifi√©es)
- [ ] **Bonus** : Authentification Kubernetes configur√©e (optionnel)

---

## PHASE 4 ‚Äî Ingress & HTTPS (cert-manager)

### Objectif
Exposer l'application de mani√®re s√©curis√©e via HTTPS avec gestion automatique des certificats TLS

### üîê √Ä quoi sert cert-manager ?

**cert-manager** est un op√©rateur Kubernetes qui automatise la gestion des certificats TLS/SSL. Dans un environnement Kubernetes, lorsque vous exposez une application via Ingress avec HTTPS, vous avez besoin de certificats TLS pour s√©curiser les communications.

**Sans cert-manager** :
- Vous devriez cr√©er manuellement les certificats TLS
- Les renouveler manuellement avant expiration
- G√©rer les Secrets Kubernetes contenant les certificats
- Risque d'interruption de service si un certificat expire

**Avec cert-manager** :
- ‚úÖ **G√©n√©ration automatique** des certificats TLS
- ‚úÖ **Renouvellement automatique** avant expiration
- ‚úÖ **Gestion des Secrets** Kubernetes automatique
- ‚úÖ **Support de plusieurs sources** : Let's Encrypt (production), auto-sign√© (dev/test), autres CAs
- ‚úÖ **D√©claration via CRD** : vous d√©clarez ce que vous voulez (Certificate), cert-manager s'occupe du reste

### üìö Concepts cl√©s

**Issuer / ClusterIssuer** :
- D√©finit **d'o√π** obtenir les certificats (source d'autorit√©)
- **Issuer** : valable pour un namespace sp√©cifique
- **ClusterIssuer** : valable pour tout le cluster
- Types : `selfSigned` (auto-sign√©), `acme` (Let's Encrypt), `ca` (autorit√© de certification interne), etc.

**Certificate** :
- Ressource Kubernetes (CRD) qui d√©clare **quel certificat** vous voulez
- Sp√©cifie : le domaine, la dur√©e de validit√©, l'Issuer √† utiliser
- cert-manager g√©n√®re automatiquement un **Secret TLS** avec le certificat

**Secret TLS** :
- Secret Kubernetes contenant le certificat et la cl√© priv√©e
- G√©n√©r√© automatiquement par cert-manager
- Utilis√© par l'Ingress pour activer HTTPS

### Ce que vous devez faire

1. **Installer cert-manager** via Helm pour b√©n√©ficier de la gestion automatique des certificats dans le namespace **cert-manager**

2. **Comprendre les CRD** install√©es par cert-manager (Issuer, Certificate, etc.) pour ma√Ætriser les ressources disponibles

3. **Cr√©er un Issuer self-signed** :
   - Pour l'examen, vous utiliserez un certificat auto-sign√© (pas de validation externe)
   - **Notes** : En production, on utiliserait un ClusterIssuer avec Let's Encrypt pour des certificats reconnus

4. **Cr√©er un Certificate** pour `app.localhost` :
   - D√©clarer le certificat souhait√©
   - cert-manager g√©n√©rera automatiquement le Secret TLS

5. **Installer un Ingress Controller** (nginx-ingress) :
   - Kubernetes ne fournit pas d'Ingress Controller par d√©faut
   - nginx-ingress est le plus populaire et le plus utilis√©

6. **Configurer l'Ingress avec TLS** :
   - Utiliser le Secret TLS g√©n√©r√© par cert-manager
   - Configurer le routage path-based : `/` ‚Üí frontend, `/api` ‚Üí backend
   - Acc√©der √† l'application via `https://app.localhost`

### Travaux demand√©s

1. **Installer cert-manager via Helm**
   - Ajouter le repo Helm cert-manager (Faites une recherche sur la documentation officielle de [cert-manager](https://cert-manager.io))
   - Installer cert-manager dans le namespace `cert-manager`
   - V√©rifier l'installation avec `kubectl get pods -n cert-manager`

2. **Identifier les CRD install√©s**
   - Lister les Custom Resource Definitions (CRD) install√©es par cert-manager :
     ```bash
     kubectl get crd | grep cert-manager
     ```
   - Comprendre leur r√¥le :
     - `issuers.cert-manager.io` / `clusterissuers.cert-manager.io` : Sources d'autorit√© pour les certificats
     - `certificates.cert-manager.io` : D√©claration des certificats souhait√©s
     - `certificaterequests.cert-manager.io` : Requ√™tes internes de certificats (g√©r√©es automatiquement)

3. **Cr√©er les ressources cert-manager**
   - Cr√©er un **Issuer** self-signed dans le namespace `todolist`
     - Type : `selfSigned` (certificat auto-sign√©, pas de validation externe)
     - Utile pour le d√©veloppement et les tests
   - Cr√©er un **Certificate** pour `app.localhost` dans le namespace `todolist`
     - R√©f√©rencer l'Issuer cr√©√© pr√©c√©demment
     - Sp√©cifier le domaine : `app.localhost`
     - cert-manager g√©n√©rera automatiquement un Secret TLS nomm√© `app-localhost-tls`

4. **V√©rifier la g√©n√©ration du certificat**
   - V√©rifier que le Certificate est pr√™t : `kubectl get certificate -n todolist`
   - V√©rifier que le Secret TLS a √©t√© cr√©√© : `kubectl get secret app-localhost-tls -n todolist`
   - Examiner les d√©tails : `kubectl describe certificate app-localhost-tls -n todolist`

5. **Installer un Ingress Controller** (si pas d√©j√† fait)
   - **Avec Minikube** : `minikube addons enable ingress`
   - **Avec un cluster standard** : Installer nginx-ingress via Helm ou kubectl
   - V√©rifier que l'Ingress Controller est actif : `kubectl get pods -n ingress-nginx`

6. **Configurer un Ingress HTTPS**
   - Cr√©er un Ingress avec TLS dans le namespace `todolist`
   - Utiliser le Secret TLS `app-localhost-tls` g√©n√©r√© par cert-manager
   - Configurer le routage path-based :
     - `/api` ‚Üí Service backend (port 8000)
     - `/` ‚Üí Service frontend (port 80)
   - Sp√©cifier l'IngressClass : `nginx`
   - Acc√©der √† l'application via `https://app.localhost`

### Notes importantes

- **Certificat auto-sign√©** : Le certificat √©tant auto-sign√©, un warning de s√©curit√© s'affichera dans le navigateur. C'est normal et attendu pour un environnement de d√©veloppement/test. En production, on utiliserait Let's Encrypt pour des certificats reconnus.

- **minikube tunnel** : Avec Minikube, vous devrez lancer `sudo -E minikube tunnel` dans un terminal s√©par√© pour exposer l'Ingress Controller (type LoadBalancer) puis y acc√©der depuis votre navigateur web.

### ‚úÖ Validation attendue

- [ ] cert-manager install√© et pods en cours d'ex√©cution
- [ ] Issuer cr√©√© et pr√™t
- [ ] Certificate cr√©√© et dans l'√©tat "Ready"
- [ ] Secret TLS `app-localhost-tls` g√©n√©r√© automatiquement par cert-manager
- [ ] Ingress Controller install√© et actif
- [ ] Ingress cr√©√© avec une adresse IP
- [ ] HTTPS fonctionnel (acc√®s via `https://app.localhost`)
- [ ] Routage correct : `/` ‚Üí frontend, `/api` ‚Üí backend
- [ ] Le certificat est visible dans les secrets du namespace

### üîó Ressources utiles

- [Documentation cert-manager](https://cert-manager.io/docs/)
- [Guide d'acc√®s Ingress](./kubernetes/ingress/ACCESS.md)
- [nginx-ingress Documentation](https://kubernetes.github.io/ingress-nginx/)

---

## PHASE 5 ‚Äî Collecte de logs (OpenSearch & Fluent Bit)

### Objectif
Centraliser et visualiser les logs du backend et du frontend avec OpenSearch et Fluent Bit

### Travaux demand√©s

1. **D√©ployer OpenSearch via Helm**
   - Ajouter le repo Helm OpenSearch
   - Installer OpenSearch dans le namespace `logging` avec Helm et utiliser le fichier de configuration `resources/values-opensearch.yaml`
   - V√©rifier que le pod OpenSearch sont en cours d'ex√©cution (**Notes** : OpenSearch met du temps √† d√©marrer)

2. **D√©ployer OpenSearch Dashboards via Helm**
   - Installer OpenSearch Dashboards via Helm dans le namespace `logging` en utilisant le fichier de configuration `resources/values-opensearch-dashboards.yaml`
   - Acc√©der √† l'interface via port-forward : `kubectl -n logging port-forward svc/opensearch-dashboards 5601:5601`

3. **D√©ployer Fluent Bit via Helm**
   - **Cr√©er un ConfigMap pour les parsers personnalis√©s** : Les logs du backend (Uvicorn) et du frontend (Nginx) ont des formats sp√©cifiques. Pour les parser correctement et extraire les champs (m√©thode HTTP, chemin, code de statut, etc.), il faut cr√©er des parsers personnalis√©s :
     ```bash
     kubectl create configmap fluent-bit-custom-parsers --from-literal=custom_parsers.conf='[PARSER]
         Name        uvicorn_access
         Format      regex
         Regex       ^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?<level>[A-Z]+):\s+(?<client_ip>[^:]+):(?<client_port>\d+)\s+-\s+"(?<method>[A-Z]+)\s+(?<path>[^ ]+)\s+(?<protocol>[^"]+)"\s+(?<status>\d{3})
         Time_Key    time
         Time_Format %Y-%m-%d %H:%M:%S

     [PARSER]
         Name        nginx_access
         Format      regex
         Regex       ^(?<remote>[^ ]+) [^ ]+ [^ ]+ \[(?<time>[^\]]+)\] "(?<method>[A-Z]+) (?<path>[^ ]+) (?<protocol>[^"]+)" (?<status>\d{3}) (?<bytes>\d+) "(?<referer>[^"]*)" "(?<agent>[^"]*)" "(?<forwarded_for>[^"]*)"
         Time_Key    time
         Time_Format %d/%b/%Y:%H:%M:%S %z' -n logging
     ```
   - Ajouter le repo Helm Fluent
   - Installer Fluent Bit via Helm dans le namespace `logging`
   - Configurer Fluent Bit en utilisant le fichier `kubernetes/logging/values-fluentbit.yaml` pour :
     - Collecter les logs des pods backend et frontend (namespace `todolist`)
     - Parser les logs avec les parsers personnalis√©s (uvicorn_access, nginx_access)
     - Envoyer les logs vers OpenSearch
   - V√©rifier que le DaemonSet Fluent Bit est d√©ploy√© sur tous les n≈ìuds

4. **Cr√©er un index pattern dans OpenSearch Dashboards**
   - Se connecter √† OpenSearch Dashboards via port-forward : `kubectl port-forward svc/opensearch-dashboards 5601:5601 -n logging`
   - Acc√©der √† l'interface dans votre navigateur : http://localhost:5601
   - Se rendre dans **"Dashboards Management"** puis **"Index patterns"**
   - Cliquer sur **"Create index pattern"**

   ![OpenSearch](images/indexpattern.png)

   - Appeler `todolist*` pour le nom de l'index pattern (avec le wildcard `*` pour inclure tous les indices commen√ßant par `todolist`)
   - Choisir `@timestamp` pour le champ de temps (Time field)
   - Cliquer sur **"Create index pattern"**

4. **V√©rifier la collecte de logs**
   - G√©n√©rer du trafic sur le backend et le frontend
   - V√©rifier que les logs sont bien collect√©s et index√©s dans OpenSearch
   - Acc√©der √† l'interface OpenSearch Dashboards puis "Discover" pour visualiser les logs
   - Filtrer les logs par namespace (`todolist`), par pod (`backend-*`, `frontend-*`)

   ![Dashbord](images/opensearch.png)

### ‚úÖ Validation attendue

- [ ] OpenSearch d√©ploy√© et fonctionnel dans le namespace `logging`
- [ ] OpenSearch Dashboards accessible et fonctionnel
- [ ] Fluent Bit d√©ploy√© et collectant les logs
- [ ] Logs du backend et du frontend visibles dans OpenSearch
- [ ] Filtrage des logs par namespace et pod fonctionnel
- [ ] Capture d'√©cran des logs dans OpenSearch Dashboards (Discover) montrant les logs du backend et du frontend

---

## Livrables attendus

### Repository Git contenant :

- [ ] Manifests Kubernetes 
- [ ] Application fonctionnelle en HTTPS 
- [ ] Monitoring op√©rationnel (Prometheus + Grafana)
- [ ] Dashboard Grafana export√© (JSON)
- [ ] Configuration OpenSearch et Fluent Bit

### Rapport expliquant :

- [ ] Chaque d√©cision technique prise
- [ ] Chaque CRD utilis√© (cert-manager, Prometheus Operator)
- [ ] Limites li√©es √† Minikube (si applicable)
- [ ] Architecture finale de l'application
- [ ] Choix de la base de donn√©es (PostgreSQL ou CockroachDB) et justification

### Captures d'√©cran :

Merci de prendre un maximum de captures d‚Äô√©cran des √©l√©ments que vous estimez utiles afin de faciliter ma correction. Voici quelques exemples :

- [ ] Interface Grafana avec le dashboard import√©
- [ ] Prometheus avec les m√©triques du backend
- [ ] OpenSearch Dashboards avec les logs du backend et frontend
- [ ] Application accessible via HTTPS sur https://app.local
- [ ] Vault avec les credentials dynamiques g√©n√©r√©s
- [ ] Vault avec les secrets Engine PostgreSQL et Kubernetes configur√©s
- [ ] Pod alpine/psql connect√© √† PostgreSQL avec identifiants dynamique avec tentative de suppression d'un √©l√©ment d'une table (montrant l'√©chec avec permissions read-only)
- [ ] Capture d'√©cran des logs dans OpenSearch Dashboards (Discover) montrant les logs du backend et du frontend

---

## Conseils et bonnes pratiques

### Structure recommand√©e pour les manifests

```
manifests/
‚îú‚îÄ‚îÄ database/               # PostgreSQL ou CockroachDB (dans namespace db)
‚îú‚îÄ‚îÄ backend/
‚îú‚îÄ‚îÄ frontend/
‚îú‚îÄ‚îÄ ingress/
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml
‚îú‚îÄ‚îÄ cert-manager/
‚îÇ   ‚îú‚îÄ‚îÄ issuer.yaml
‚îÇ   ‚îî‚îÄ‚îÄ certificate.yaml
‚îú‚îÄ‚îÄ monitoring/   
‚îú‚îÄ‚îÄ logging/
```

### Commandes utiles

```bash
# V√©rifier l'√©tat des pods
kubectl get pods -n <namespace>

# Voir les logs
kubectl logs -f <pod-name> -n <namespace>

# D√©crire une ressource
kubectl describe <resource-type> <resource-name> -n <namespace>

# V√©rifier les √©v√©nements
kubectl get events -n <namespace> --sort-by='.lastTimestamp'

# Port-forward pour tester
kubectl port-forward svc/<service-name> <local-port>:<service-port> -n <namespace>
```
---

## Ressources

- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [Helm Documentation](https://helm.sh/docs/)
- [Prometheus Operator](https://prometheus-operator.dev/)
- [cert-manager Documentation](https://cert-manager.io/docs/)
- [OpenSearch Documentation](https://opensearch.org/docs/)
- [OpenSearch Helm Charts](https://github.com/opensearch-project/helm-charts)
- [Fluent Bit Documentation](https://docs.fluentbit.io/)
- [Fluent Bit Helm Charts](https://github.com/fluent/helm-charts)
- [Hashicorp Vault Documentation](https://developer.hashicorp.com/vault/docs)
- [Vault Helm Charts](https://github.com/hashicorp/vault-helm)
- [CockroachDB Documentation](https://www.cockroachlabs.com/docs/) (optionnel)

---

Bon courage et n'h√©sitez pas √† me contacter si besoin !
Pour les int√©r√©ss√©s, une session de correction de l'examen sera organis√©e.

---

BOUNACEUR Mehdi

